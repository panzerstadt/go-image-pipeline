local: go run ./cmd/dev
producer: go run ./cmd/producer
consumer: go run ./cmd/consumer
kafka: docker compose up
ui: localhost:8080

### producer

- we currently have one producer. its a golang loop with a 10 second sleep timer that goes through all files in a specified folder and sends a message to the topic

### consumers and consumer groups

- we can spin up as many consumers as we want in the consumer group. but only one consumer will be picked for one partition
- e.g. we want to see more than one consumer from the consumer group running, we should set >1 partition for the topic

### Goal: See how consumers in the same group divide work using partitions.

- how:

  - create topic with 2 partitions
  - start 3 consumers for the same consumer group
  - run the producer

- note:

  - when you stop kafka, make sure you stop the consumers too. otherwise they don't get assigned to partitions properly somehow

### producing messages with custom partition keys

```
msg := &sarama.ProducerMessage{
    Topic: "your-topic",
    Key:   sarama.StringEncoder("user123"), // This determines the partition
    Value: sarama.StringEncoder("some image job payload"),
}

partition, offset, err := producer.SendMessage(msg)
if err != nil {
    log.Fatalf("failed to send message: %v", err)
}

log.Printf("Message sent to partition %d at offset %d", partition, offset)
```

#### create a new consumer group -> maybe notifications

Try this:
â€¢ Reuse the same topic
â€¢ Spin up a new consumer group (e.g., notification-worker) and analytics worker
â€¢ It should receive all messages from offset 0, even if your image-processing group already did
â€¢ This simulates a fan-out architecture

TODO:

#### find out what else can kafka do that is worth exploring

Here are some spicy ideas:
â€¢ Dead Letter Queues (DLQs) â€“ catch poison messages that keep failing
â€¢ Delayed delivery via scheduled retries (not native, but doable with workarounds)
â€¢ Stream processing using something like Kafka Streams or Faust (Python)
â€¢ Log compaction vs normal retention
â€¢ Message headers for metadata transport

#### maybe try big loads

Goal: Stress test for throughput and backpressure

Try this:
â€¢ Bulk-produce 10,000+ messages quickly
â€¢ Monitor how your consumers handle lag (check ConsumerLag via Kafka UI or metrics)
â€¢ Optional: add artificial delay in consumer to simulate heavy processing

#### how do i DDD? the encapsulation of items -> ask chatgpt

This is a big one. At a high level:
â€¢ Entities: Have identity (e.g., ImageJob) and mutable state
â€¢ Value Objects: No identity, just data (e.g., Resolution, Dimensions)
â€¢ Aggregates: Cluster of objects treated as a unit (e.g., an ImageProcessingWorkflow)
â€¢ Repositories: Abstract away data access
â€¢ Services: Contain business logic that spans multiple aggregates

For your image pipeline, you might:
â€¢ Turn a ResizeJob into an aggregate
â€¢ Have JobStatus as a value object
â€¢ Use a repository interface to load/save job data (e.g., from Kafka state, S3, or DB)

ğŸ’¡ Breakdown
â€¢ imagejob: your domain modelâ€”aggregates, value objects, behavior
â€¢ processing: application layerâ€”services that orchestrate the domain
â€¢ repository: infrastructure layerâ€”Kafka or FS adapters for persistence
â€¢ cmd: executables (Goâ€™s standard approach)
â€¢ internal: keeps implementation details encapsulated, Go idiomatic

```
go-image-pipeline/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ consumer/               # Entry point for consumer service
â”‚   â”œâ”€â”€ producer/               # Entry point for producer
â”‚   â””â”€â”€ dev/                    # Dev/testing runner
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ imagejob/               # Aggregate: ResizeJob, JobState transitions
â”‚   â”‚   â”œâ”€â”€ job.go              # Entity definition (ID, state, etc.)
â”‚   â”‚   â””â”€â”€ status.go           # Value object: JobStatus, e.g. Pending, Done
â”‚   â”œâ”€â”€ processing/             # Service: contains business logic (e.g., ResizeImage)
â”‚   â”‚   â””â”€â”€ service.go
â”‚   â”œâ”€â”€ repository/             # Interfaces and adapters for persistence
â”‚   â”‚   â”œâ”€â”€ interface.go        # Repository interfaces
â”‚   â”‚   â”œâ”€â”€ kafka_repo.go       # Implementation: writes/reads from Kafka
â”‚   â”‚   â””â”€â”€ fs_repo.go          # (Optional) Implementation: reads from local FS
â”‚   â””â”€â”€ shared/                 # Utilities, domain-wide constants, error types
â”‚       â””â”€â”€ logger.go
â”œâ”€â”€ proto/                      # Protobuf definitions
â”œâ”€â”€ ui/                         # Optional: frontend or dashboard
â”œâ”€â”€ scripts/                    # Local tooling, CLI commands
â”œâ”€â”€ README.md
â””â”€â”€ go.mod
```

#### DDD

âœ… 3. Separate domain logic from coordination + plumbing
â€¢ Domain logic: rules, decisions, calculations
â€¢ Application logic: orchestration, calling services, scheduling
â€¢ Infrastructure: Kafka, DB, HTTP, Docker, etc.

For instance:
â€¢ The rule â€œonly resize images wider than 800pxâ€ â†’ domain
â€¢ The act of pulling that job from Kafka â†’ infra

#### domain: Image Processing Workflow Management

Which means your domain model includes:
â€¢ Jobs (entities)
â€¢ Status/state transitions (value objects or enums)
â€¢ Dimensions, file types (value objects)
â€¢ Processing decisions and constraints (business rules)

Everything elseâ€”how you send a job through Kafka, where you store images, even the UIâ€”is supporting infrastructure.

### DDD

- if the domain is about image processing workflow mgmt
- then the domain object is not the jpeg, but the imageJob

ğŸ“š Core DDD Vocabulary

These are terms youâ€™ll see in DDD-oriented systems:

ğŸ§± Domain Layer
â€¢ Entity â€“ has identity (e.g. ImageJob, User)
â€¢ Value Object â€“ no identity, immutable (e.g. Dimensions, Resolution)
â€¢ Aggregate â€“ cluster of entities/VOs with rules (e.g. ResizeJob with status and constraints)
â€¢ Domain Service â€“ stateless service with domain logic across multiple entities (e.g. JobScheduler)
â€¢ Domain Event â€“ something that happened in the domain (e.g. ImageResizedEvent)

ğŸ§© Application Layer
â€¢ Application Service / Use Case â€“ orchestrates domain behavior (e.g. ProcessImageUseCase)
â€¢ Command â€“ request to perform an action (e.g. ResizeImageCommand)
â€¢ Query â€“ read model request
â€¢ DTO â€“ data transfer object (input/output, not a domain model)

ğŸ“¦ Infrastructure Layer
â€¢ Repository â€“ access to domain objects (e.g. Kafka, FS, DB)
â€¢ Adapter â€“ a plug-in that conforms to interface (e.g. KafkaProducerAdapter)
â€¢ Gateway â€“ external service abstraction (e.g. S3Client)

### MVC <-> DDD

| Common Term | DDD Equivalent (ish)                           |
| ----------- | ---------------------------------------------- |
| Model       | Often includes Entities + Value Objects        |
| Repository  | Same in DDD                                    |
| Service     | Could be Domain Service or Application Service |
| Controller  | UI/Application Layer Entry Point               |

### ğŸ” Tips to Recognize DDD-style Structure

Look for:
â€¢ Entity + Value Object split
â€¢ Interfaces for repositories
â€¢ UseCase or CommandHandler classes/files
â€¢ Language like Domain, Aggregate, Event
â€¢ Folders like domain, application, infrastructure, interfaces
